{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MiniImageNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1r3HGWCUdX19WRiQliGAnuqiWXZMNkxtu",
      "authorship_tag": "ABX9TyOXwFKQPytFEI2NNbKilvbP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DVAEsCL/DVAEsCL/blob/main/MiniImageNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mLGj-oIJcie"
      },
      "source": [
        "#Download the miniImageNet dataset\r\n",
        "Please click the [link](https://drive.google.com/drive/folders/1-kwTWeyrw3uetDV_Y2xRuH5fJI3qq26h?usp=sharing) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRXAp7jSYHwi"
      },
      "source": [
        "import pickle\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "def dataprocess(data_path):\r\n",
        "  with open(data_path, 'rb') as fopen:\r\n",
        "     #contents = np.load(fopen, allow_pickle=True, encoding='bytes')\r\n",
        "    contents = np.load(fopen, allow_pickle=True, encoding='latin1')\r\n",
        "    return contents\r\n",
        "test_in = dataprocess(\"/content/drive/MyDrive/ColabNotebooks/ContinualLearning/miniImageNet/miniImageNet_full.pickle\")"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFKkdbbbYN7T",
        "outputId": "57b56a96-ba12-4d1d-c55e-66ffbb6f22dd"
      },
      "source": [
        "data = test_in['images']\r\n",
        "labels = test_in['labels']\r\n",
        "print(test_in['images'].shape)\r\n",
        "print(test_in['labels'].shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 84, 84, 3)\n",
            "(60000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJOIieXNYWKJ"
      },
      "source": [
        "indices = []\r\n",
        "for i in range(100):\r\n",
        "  index = np.where(labels == i)[0].tolist()\r\n",
        "  indices.append(index)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8BkJrWmt8nC"
      },
      "source": [
        "new_indices_train = []\r\n",
        "new_indices_test = []\r\n",
        "val = 0\r\n",
        "for i in range(20):\r\n",
        "  for j in range(5):\r\n",
        "    if j == 0:\r\n",
        "      ind1 = np.array(indices[j + val][:500])\r\n",
        "      ind2 = np.array(indices[j + val][500:])\r\n",
        "    else:\r\n",
        "      ind1 = np.concatenate((ind1, indices[j + val][:500]), axis = 0)\r\n",
        "      ind2 = np.concatenate((ind2, indices[j + val][500:]), axis = 0) #.tolist()\r\n",
        "  #print(len(ind1))\r\n",
        "  new_indices_train.append(ind1.tolist())\r\n",
        "  new_indices_test.append(ind2.tolist())\r\n",
        "  val = val + j + 1"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtImqQ2M1HbJ"
      },
      "source": [
        "import torch\r\n",
        "traindata = []\r\n",
        "trainlabels = []\r\n",
        "testdata = []\r\n",
        "testlabels = []\r\n",
        "\r\n",
        "for i in range(20):\r\n",
        "  traindata.append(torch.from_numpy(data[new_indices_train[i]].astype(np.float32)/255).view(2500, 3, 84, 84))\r\n",
        "  trainlabels.append(torch.from_numpy(labels[new_indices_train[i]]))\r\n",
        "  testdata.append(torch.from_numpy(data[new_indices_test[i]].astype(np.float32)/255).view(500, 3, 84, 84))\r\n",
        "  testlabels.append(torch.from_numpy(labels[new_indices_test[i]]))\r\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2O9nnXx1JYg",
        "outputId": "29c634c9-331e-4df4-eb3b-9bb61ebc3fa0"
      },
      "source": [
        "print(testdata[0].shape)\r\n",
        "print(traindata[0].shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([500, 3, 84, 84])\n",
            "torch.Size([2500, 3, 84, 84])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tes082Ud2tte"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "class encoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(encoder, self).__init__()\r\n",
        "    self.conv1 = nn.Conv2d(3, 6, 3, 2, 1) \r\n",
        "    self.conv2 = nn.Conv2d(6, 12, 2, 2, 0)\r\n",
        "    self.conv3 = nn.Conv2d(12, 24, 2, 2, 0)\r\n",
        "    self.conv4 = nn.Conv2d(24, 48, 2, 2, 0)\r\n",
        "    self.conv5 = nn.Conv2d(48, 48, 2, 2, 0)\r\n",
        "  def forward(self, x):\r\n",
        "    x = self.conv1(x)\r\n",
        "    x = self.conv2(x)\r\n",
        "    x = self.conv3(x)\r\n",
        "    x = self.conv4(x)\r\n",
        "    x = self.conv5(x)\r\n",
        "    return x\r\n",
        "class decoder(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super(decoder, self).__init__()\r\n",
        "    self.nc_mnist = 118\r\n",
        "    self.nc_cifar10 = 202\r\n",
        "    self.nc_cifar100 = 292\r\n",
        "    self.nk_mnist = 3\r\n",
        "    self.nk_cifar10 = 4\r\n",
        "    self.decon1 = nn.ConvTranspose2d(196, 24, 3, 1, 0)\r\n",
        "    self.decon2 = nn.ConvTranspose2d(24, 12, 4, 2, 0)\r\n",
        "    self.decon3 = nn.ConvTranspose2d(12, 6, 7, 2, 0)\r\n",
        "    self.decon4 = nn.ConvTranspose2d(6, 3, 2, 2, 0)\r\n",
        "    self.decon5 = nn.ConvTranspose2d(3, 3, 2, 2, 0)\r\n",
        "  def forward(self, x):\r\n",
        "    #print(x.shape, '11')\r\n",
        "    x = x.view(x.shape[0], 196, 1, 1)\r\n",
        "    x = self.decon1(x)\r\n",
        "    x = self.decon2(x)\r\n",
        "    x = self.decon3(x)\r\n",
        "    x = self.decon4(x)\r\n",
        "    x = self.decon5(x)\r\n",
        "    return x\r\n",
        "\r\n",
        "class VAE(nn.Module):\r\n",
        "  def __init__(self, eps):\r\n",
        "    super(VAE, self).__init__()\r\n",
        "    self.en = encoder()\r\n",
        "    self.de = decoder()\r\n",
        "    self.eps = eps\r\n",
        "    self.mnist_z = 108\r\n",
        "    self.cifar10_z = 192\r\n",
        "  def forward(self, x, one_hot):\r\n",
        "    x = self.en(x)\r\n",
        "    x = x.view(x.shape[0], -1)\r\n",
        "    mu = x[:, :96]\r\n",
        "    logvar = x[:, 96:]\r\n",
        "    std = torch.exp(0.5 * logvar)\r\n",
        "    z = mu + self.eps * std\r\n",
        "    #print(z.shape, 'aaa', one_hot.shape)\r\n",
        "    z1 = torch.cat((z, one_hot), axis = 1)\r\n",
        "    #print(z1.shape, 'bbb')\r\n",
        "    return self.de(z1), mu, logvar\r\n",
        "\r\n",
        "class private(nn.Module):\r\n",
        "  def __init__(self, eps):\r\n",
        "    super(private, self).__init__()\r\n",
        "    self.task = torch.nn.ModuleList()\r\n",
        "    self.eps = eps\r\n",
        "    for _ in range(20):\r\n",
        "      self.task.append(VAE(self.eps))\r\n",
        "\r\n",
        "  def forward(self, x, one_hot, task_id):\r\n",
        "    return self.task[task_id].forward(x, one_hot)\r\n",
        "\r\n",
        "class NET(nn.Module):\r\n",
        "  def __init__(self, eps):\r\n",
        "    super(NET, self).__init__()\r\n",
        "    self.eps = eps\r\n",
        "    self.shared = VAE(self.eps)\r\n",
        "    self.private = private(self.eps)\r\n",
        "    self.head = torch.nn.ModuleList()\r\n",
        "    self.mnist = 216\r\n",
        "    self.cifar10 = 384\r\n",
        "    self.in_mnist = 2\r\n",
        "    self.in_cifar10 = 6\r\n",
        "    for _ in range(20):\r\n",
        "      self.head.append(\r\n",
        "          nn.Sequential(\r\n",
        "            nn.Conv2d(6, 12, 3, 2, 1),\r\n",
        "            #nn.Conv2d(3, 6, 2, 2, 0),\r\n",
        "            #nn.Conv2d(6, 12, 2, 2, 0),\r\n",
        "            nn.Conv2d(12, 24, 2, 2, 0),\r\n",
        "            nn.Conv2d(24, 48, 2, 2, 0),\r\n",
        "            nn.Conv2d(48, 48, 2, 2, 0),\r\n",
        "            nn.Flatten(1, -1),\r\n",
        "            nn.Linear(48*5*5, 100), #for cifar10 only\r\n",
        "            #nn.Linear(100, 10)\r\n",
        "          )\r\n",
        "      )\r\n",
        "\r\n",
        "  def forward(self, x, one_hot, task_id):\r\n",
        "    s_x, s_mu, s_logvar = self.shared(x, one_hot)\r\n",
        "    p_x, p_mu, p_logvar = self.private(x, one_hot, task_id)\r\n",
        "    #print(s_x.shape, p_x.shape, '111')\r\n",
        "    x = torch.cat([s_x, p_x], dim = 1)\r\n",
        "    #print(x.shape, '22')\r\n",
        "    return self.head[task_id].forward(x), (s_x, s_mu, s_logvar), (p_x, p_mu, p_logvar)\r\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVOAFiBKKEhA"
      },
      "source": [
        "#Number of epochs and synthetic data\r\n",
        "If you wish to change the number of epochs and synthetic data used as a generative replay, check lines 170 and 70, respectively. Change according to your requirments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIL06f3r6SzQ"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.autograd import Variable\r\n",
        "import numpy as np\r\n",
        "from collections import deque\r\n",
        "from torch.autograd import grad as torch_grad\r\n",
        "\r\n",
        "import torchvision.utils as vutils\r\n",
        "\r\n",
        "import os\r\n",
        "import os.path\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "np.set_printoptions(threshold=np.inf)\r\n",
        "\r\n",
        "\r\n",
        "class CL_VAE():\r\n",
        "  def __init__(self):\r\n",
        "    super(CL_VAE, self).__init__()\r\n",
        "\r\n",
        "    self.batch_size = 64\r\n",
        "    self.mnist_z = 108\r\n",
        "    self.cifar10_z = 192\r\n",
        "    self.num_class_cifar100 = 100\r\n",
        "    self.num_class_cifar10 = 10\r\n",
        "    self.build_model()\r\n",
        "    self.set_cuda()\r\n",
        "    self.criterion = torch.nn.CrossEntropyLoss()\r\n",
        "    self.recon = torch.nn.MSELoss()\r\n",
        "    self.net_path = 'path/trial.pth'\r\n",
        "    self.accuracy_matrix = [[] for kk in range(20)]\r\n",
        "    self.acc_matr = []\r\n",
        "    self.forget_mat = []\r\n",
        "    self.acc_25 = []\r\n",
        "    self.acc_50 = []\r\n",
        "\r\n",
        "\r\n",
        "  def build_model(self):\r\n",
        "    self.eps = torch.randn(self.batch_size, 96)\r\n",
        "    self.eps = self.eps.cuda()\r\n",
        "    self.net = NET(self.eps)\r\n",
        "    pytorch_total_params = sum(p.numel() for p in self.net.parameters() if p.requires_grad)\r\n",
        "    print('pytorch_total_params:', pytorch_total_params)\r\n",
        "    \r\n",
        "  def set_cuda(self):\r\n",
        "    self.net.cuda()\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "  def VAE_loss(self, recon, mu, sigma):\r\n",
        "    kl_div = -0.5 * torch.sum(1 + sigma - mu.pow(2) - sigma.exp())\r\n",
        "    #print('kl_div', kl_div.item())\r\n",
        "    return recon + kl_div\r\n",
        "\r\n",
        "  def train(self, all_traindata, all_trainlabels, all_testdata, all_testlabels, total_tasks):\r\n",
        "\r\n",
        "    replay_classes = []\r\n",
        "    for i in range(total_tasks):\r\n",
        "      traindata = all_traindata[i]\r\n",
        "      trainlabels = all_trainlabels[i]\r\n",
        "      testdata = all_testdata[i]\r\n",
        "      testlabels = all_testlabels[i]\r\n",
        "      #print(trainlabels, 'avfr')\r\n",
        "      replay_classes.append(sorted(list(set(trainlabels.numpy().tolist()))))\r\n",
        "      if i + 1 == 1:\r\n",
        "        self.train_task(traindata, trainlabels, testdata, testlabels, i)\r\n",
        "        #replay_classes.append(sorted(list(set(trainlabels.detach().numpy().tolist()))))\r\n",
        "      else:\r\n",
        "        num_gen_samples = 4\r\n",
        "        #z_dim = 108\r\n",
        "        for m in range(i):\r\n",
        "          #print(replay_classes, 'replay_classes')\r\n",
        "          replay_trainlabels = []\r\n",
        "          for ii in replay_classes[m]:\r\n",
        "            for j in range(num_gen_samples):\r\n",
        "              replay_trainlabels.append(ii)\r\n",
        "          replay_trainlabels = torch.tensor(replay_trainlabels)\r\n",
        "          replay_trainlabels_onehot = self.one_hot(replay_trainlabels)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "          z = torch.randn(5 * num_gen_samples, 96)\r\n",
        "          #print(z.shape, replay_trainlabels_onehot.shape, 'aa')\r\n",
        "          z_one_hot = torch.cat((z, replay_trainlabels_onehot), axis = 1)\r\n",
        "          z_one_hot = z_one_hot.cuda()\r\n",
        "          replay_data = self.net.private.task[m].de(z_one_hot).detach().cpu()\r\n",
        "\r\n",
        "          traindata = torch.cat((replay_data, traindata), axis = 0)\r\n",
        "          trainlabels = torch.cat((replay_trainlabels, trainlabels))\r\n",
        "          testdata = torch.cat((testdata, torch.tensor(all_testdata[m])), axis = 0)\r\n",
        "          testlabels = torch.cat((testlabels, torch.tensor(all_testlabels[m])))\r\n",
        "      \r\n",
        "        self.train_task(traindata, trainlabels, testdata, testlabels, i)\r\n",
        "      self.acc_mat(all_testdata, all_testlabels, total_tasks, i)\r\n",
        "      \r\n",
        "      #print(sorted(list(set(trainlabels.detach().numpy()))), '/n', sorted(list(set(testlabels.detach().numpy()))))\r\n",
        "    self.forgetting_measure(self.accuracy_matrix, total_tasks)\r\n",
        "    #print(np.mean(self.forget_mat), 'forget_mat:', self.forget_mat)\r\n",
        "    #print(self.acc_25, 'acc_25', np.mean(self.acc_25))\r\n",
        "    #print(self.acc_50, 'acc_50', np.mean(self.acc_50))\r\n",
        "\r\n",
        "\r\n",
        "  def one_hot(self, labels):\r\n",
        "    matrix = torch.zeros(len(labels), 100)\r\n",
        "    rows = np.arange(len(labels))\r\n",
        "    matrix[rows, labels] = 1\r\n",
        "    return matrix \r\n",
        "  def  forgetting_measure(self, accuracy_matrix, num_tasks):\r\n",
        "    forgetting_measure = []\r\n",
        "    accuracy_matrix = np.array(accuracy_matrix)\r\n",
        "    for after_task_idx in range(1, num_tasks):\r\n",
        "      after_task_num = after_task_idx + 1\r\n",
        "      #print(accuracy_matrix, 'accuracy_matrix')\r\n",
        "      prev_acc = accuracy_matrix[:after_task_num - 1, :after_task_num - 1]\r\n",
        "      forgettings = prev_acc.max(axis=0) - accuracy_matrix[after_task_num - 1, :after_task_num - 1]\r\n",
        "      forgetting_measure.append(np.mean(forgettings).item())\r\n",
        "    \r\n",
        "    #print('forgetting_measure', forgetting_measure)\r\n",
        "    #print(\"the forgetting measure is...\", np.mean(np.array(forgetting_measure)))\r\n",
        "\r\n",
        "  def acc_mat(self, testData1, testLabels1, num_tasks, t):\r\n",
        "    for kk in range(num_tasks):\r\n",
        "      testData_tw = testData1[kk]\r\n",
        "      testLabels_tw = testLabels1[kk]\r\n",
        "      testLabels_tw_classes = sorted(list(set(testLabels_tw.detach().numpy().tolist())))\r\n",
        "      #pred_tw = (class_appr.test(testData_tw)).cpu() #classifier.predict(testData_tw)\r\n",
        "      _, pred_tw = self.evall(testData_tw, testLabels_tw, kk)\r\n",
        "      #pred_tw = torch.argmax(pred_tw, dim = 1) \r\n",
        "      #pred_tw = pred_tw.cpu()       \r\n",
        "      testLabels_tw = testLabels_tw.detach().numpy()[:pred_tw.shape[0]]\r\n",
        "      #print(pred_tw[0], '12', testLabels_tw[0])\r\n",
        "      dict_correct_tw = {}\r\n",
        "      dict_total_tw = {}\r\n",
        "\r\n",
        "      for ii in testLabels_tw_classes:\r\n",
        "        dict_total_tw[ii] = 0\r\n",
        "        dict_correct_tw[ii] = 0\r\n",
        "\r\n",
        "      for ii in range(0, testLabels_tw.shape[0]):\r\n",
        "        #print(testLabels_tw[ii],'aaa', pred_tw[ii])\r\n",
        "        if(testLabels_tw[ii] == pred_tw[ii]):\r\n",
        "          dict_correct_tw[testLabels_tw[ii].item()] = dict_correct_tw[testLabels_tw[ii].item()] + 1\r\n",
        "        #print(testLabels_tw[ii], '1', dict_total_tw[testLabels_tw[ii]], '2', dict_total_tw[testLabels_tw[ii]])\r\n",
        "        dict_total_tw[testLabels_tw[ii].item()] = dict_total_tw[testLabels_tw[ii].item()] + 1\r\n",
        "            \r\n",
        "      avgAcc_tw = 0.0\r\n",
        "      num_seen_tw = 0.0\r\n",
        "        \r\n",
        "      for ii in testLabels_tw_classes:\r\n",
        "        avgAcc_tw = avgAcc_tw + (dict_correct_tw[ii]*1.0)/(dict_total_tw[ii])\r\n",
        "        num_seen_tw = num_seen_tw + 1\r\n",
        "        \r\n",
        "        avgAcc_tw = avgAcc_tw/num_seen_tw\r\n",
        "        \r\n",
        "        #testData_tw[jj].append(avgAcc_tw)\r\n",
        "      self.accuracy_matrix[t].append(avgAcc_tw)\r\n",
        "      \r\n",
        "\r\n",
        "  def model_save(self):\r\n",
        "    torch.save(self.net.state_dict(), os.path.join(self.net_path))\r\n",
        "\r\n",
        "\r\n",
        "  def train_task(self, traindata, trainlabels, testdata, testlabels, task_id):\r\n",
        "\r\n",
        "    net_opti = torch.optim.Adam(self.net.parameters(), lr = 1e-4)\r\n",
        "    #data, label = traindata\r\n",
        "    #batch_size = 64\r\n",
        "    num_iterations = int(traindata.shape[0]/self.batch_size)\r\n",
        "    num_epochs = 50\r\n",
        "    #print(num_iterations, '451')\r\n",
        "    for e in range(num_epochs):\r\n",
        "      for i in range(num_iterations):\r\n",
        "        self.net.zero_grad()\r\n",
        "        self.net.train()\r\n",
        "        \r\n",
        "\r\n",
        "        batch_data = traindata[i * self.batch_size : (i + 1)*self.batch_size]\r\n",
        "        #print(batch_data.shape, '41')\r\n",
        "        batch_label = trainlabels[i * self.batch_size : (i + 1)*self.batch_size]\r\n",
        "        batch_label_one_hot = self.one_hot(batch_label)\r\n",
        "        batch_data = batch_data.cuda()\r\n",
        "        batch_label = batch_label.cuda()\r\n",
        "        batch_label_one_hot = batch_label_one_hot.cuda()\r\n",
        "\r\n",
        "        out, shared_out, private_out = self.net(batch_data, batch_label_one_hot, task_id)\r\n",
        "        s_x, s_mu, s_logvar = shared_out\r\n",
        "        p_x, p_mu, p_logvar = private_out\r\n",
        "        #print(out.shape, '12', batch_label.shape, s_x.shape)\r\n",
        "\r\n",
        "        cross_en_loss = self.criterion(out, batch_label)\r\n",
        "\r\n",
        "        s_recon = self.recon(batch_data, s_x)\r\n",
        "        p_recon = self.recon(batch_data, p_x)\r\n",
        "\r\n",
        "        s_VAE_loss = self.VAE_loss(s_recon, s_mu, s_logvar)\r\n",
        "        p_VAE_loss = self.VAE_loss(p_recon, p_mu, p_logvar)\r\n",
        "\r\n",
        "        all_loss = cross_en_loss + s_VAE_loss + p_VAE_loss\r\n",
        "\r\n",
        "        all_loss.backward(retain_graph=True)\r\n",
        "        net_opti.step()\r\n",
        "      #print('epoch:', e + 1, 'task_loss', cross_en_loss.item(), 's_VAE:', s_VAE_loss.item(), 'p_VAE', p_VAE_loss.item())\r\n",
        "\r\n",
        "      if (e + 1) % 25 == 0:\r\n",
        "        acc1, _ = self.evall(testdata, testlabels, task_id)\r\n",
        "\r\n",
        "      if e + 1 == 25:\r\n",
        "        self.acc_25.append(acc1)\r\n",
        "      if e + 1 == 50:\r\n",
        "        self.acc_50.append(acc1)\r\n",
        "        \r\n",
        "        #acc2 = self.ev((a, b), task_id)\r\n",
        "        #print('Task:', task_id + 1, 'acc', acc1)\r\n",
        "        \r\n",
        "          \r\n",
        "        if task_id + 1 == 20:\r\n",
        "          self.model_save()\r\n",
        "    \r\n",
        "    \r\n",
        "    #self.acc_matr.append(acc1)\r\n",
        "\r\n",
        "  def evall(self, testdata, testlabels, task_id):\r\n",
        "    self.net.eval()\r\n",
        "    #data, labels = testdata\r\n",
        "    #print(testdata.shape, '11', testlabels.shape)\r\n",
        "    #batch_size = 64\r\n",
        "    num_iterations = int(testdata.shape[0]/self.batch_size)\r\n",
        "    acc = []\r\n",
        "    pred_labels_list = []\r\n",
        "    for i in range(num_iterations):\r\n",
        "      batch_data = testdata[i * self.batch_size : (i + 1) * self.batch_size]\r\n",
        "      batch_labels = testlabels[i * self.batch_size : (i + 1) * self.batch_size]\r\n",
        "      batch_label_one_hot = self.one_hot(batch_labels)\r\n",
        "      batch_data = batch_data.cuda()\r\n",
        "      batch_labels = batch_labels.cuda()\r\n",
        "      batch_label_one_hot = batch_label_one_hot.cuda()\r\n",
        "      out, _, _ = self.net(batch_data, batch_label_one_hot, task_id)\r\n",
        "      pred_labels = torch.argmax(out, axis = 1)\r\n",
        "      pred_labels_list.append(pred_labels.detach().cpu().numpy().tolist())\r\n",
        "      #print(pred_labels, 'aa')\r\n",
        "      #print(pred_labels.shape, '1452', batch_labels)\r\n",
        "      acc.append((torch.sum(batch_labels == pred_labels)/batch_data.shape[0] * 100).detach().cpu().numpy().tolist())\r\n",
        "\r\n",
        "    #print('acc:', acc)\r\n",
        "    return np.mean(np.array(acc)), np.array(pred_labels_list).flatten()\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jrNT8oT6ZUP",
        "outputId": "07589a8b-f0ac-4c34-c895-06b73bc243d2"
      },
      "source": [
        "import time\r\n",
        "aaa = CL_VAE()\r\n",
        "#start = time.time()\r\n",
        "\r\n",
        "aaa.train(traindata, trainlabels, testdata, testlabels, 20)\r\n",
        "#end = time.time()\r\n",
        "#print('It took: ', end - start)\r\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch_total_params: 4107152\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:91: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}